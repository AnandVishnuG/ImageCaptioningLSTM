{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2OhRz4SFqpSlOj4lYu+Uv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnandVishnuG/ImageCaptioningLSTM/blob/main/ImageCaptioningLSTMTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3ocyQNXPTnl",
        "outputId": "b73363ac-9c2e-4260-ab9f-5dc25eb7ace6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n",
            "Collecting transformers>=3.0.0 (from bert_score)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (16.0.5)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=3.0.0->bert_score)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=3.0.0->bert_score)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert_score) (2023.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, bert_score\n",
            "Successfully installed bert_score-0.3.13 huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from google.colab import files, drive\n",
        "from tqdm import tqdm \n",
        "from PIL import Image\n",
        "from pickle import dump, load\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Reshape\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import bert_score"
      ],
      "metadata": {
        "id": "qD87r2RdPmEz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google drive for accessing files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF3zdxsoRWL3",
        "outputId": "badde5dc-90f4-4061-bf76-315c5e53affc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of images as keys and each image's descriptions as items\n",
        "def data_to_dictionary(data):\n",
        "    descriptions = {}\n",
        "    for idx, row in data.iterrows():\n",
        "        if row[\"image\"] not in descriptions:\n",
        "            descriptions.setdefault(row[\"image\"], []).append(row[\"caption\"])\n",
        "        else:\n",
        "            descriptions[row[\"image\"]].append(row[\"caption\"])\n",
        "    return descriptions\n",
        "#converting dictionary to clean list of descriptions\n",
        "def dict_to_list(descriptions):\n",
        "    all_desc = []\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc    \n",
        "# Converting captions to tokens\n",
        "def list_to_tokens(desc_list):\n",
        "  tokens = []\n",
        "  for desc in desc_list:\n",
        "    tokens.append(word_tokenize(desc))\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "eOnElNF3TSU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Captions file \n",
        "captions = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/captions_clean.txt\",encoding=\"cp1252\")\n",
        "# Create descriptions dictionary containing image keys and corresponding captions\n",
        "descriptions = data_to_dictionary(captions)\n"
      ],
      "metadata": {
        "id": "6lZlsExBUUPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract image features using Xception model and save it\n",
        "def extract_features():\n",
        "        model = Xception( include_top=False, pooling='avg' )\n",
        "        # datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
        "        # datagen = ImageDataGenerator(rotation_range=20, horizontal_flip=True)\n",
        "        datagen = ImageDataGenerator(brightness_range=(0.5, 1.5))\n",
        "            \n",
        "        features = {}\n",
        "        for img in tqdm(os.listdir(\"/content/drive/MyDrive/Colab Notebooks/data/Images\")):\n",
        "            filename = \"/content/drive/MyDrive/Colab Notebooks/data/Images/\"  + img\n",
        "            image = Image.open(filename)\n",
        "            image = np.expand_dims(image, axis=0)\n",
        "             # Generate augmented images\n",
        "            aug_iter = datagen.flow(image, batch_size=1)\n",
        "            aug_images = [next(aug_iter)[0].astype(np.uint8) for _ in range(2)]\n",
        "            aug_images = [aug_image/127.5 - 1 for aug_image in aug_images]\n",
        "\n",
        "            for idx, aug_image in enumerate(aug_images):\n",
        "              aug_image = np.expand_dims(aug_image, axis=0)\n",
        "              feature = model.predict(aug_image, verbose=0)\n",
        "              features[f\"{img}_{idx}\"] = feature\n",
        "        return features\n",
        "features = extract_features()\n",
        "dump(features, open(\"features_aug2_brightness.p\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "a4AjxdI2Ua4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate augmented images\n",
        "datagen = ImageDataGenerator(brightness_range=(0.5, 1.5),channel_shift_range=50)\n",
        "        \n",
        "image = Image.open(\"/content/drive/MyDrive/Colab Notebooks/data/Images/111537222_07e56d5a30.jpg\")\n",
        "image = np.expand_dims(image, axis=0)\n",
        "            \n",
        "aug_iter = datagen.flow(image, batch_size=1)\n",
        "aug_images = [next(aug_iter)[0].astype(np.uint8) for _ in range(2)]\n",
        "aug_images = [aug_image/127.5 - 1 for aug_image in aug_images]\n",
        "# Display images\n",
        "for img in aug_images:\n",
        "  plt.imshow(img)\n",
        "  plt.show(block=True)"
      ],
      "metadata": {
        "id": "zzz1_uA4JPlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_photos(filename):\n",
        "    file = open(filename, 'r')\n",
        "    photos = file.read().split(\"\\n\")[:-1]\n",
        "    return photos\n",
        "def load_descriptions(photos):\n",
        "    d = {}\n",
        "    for photo in photos:\n",
        "        if photo in descriptions.keys():\n",
        "            if photo not in d.keys():\n",
        "                l = []            \n",
        "                for desc in descriptions[photo]:\n",
        "                    l.append(\"<start> \" + desc + \" <end>\" ) \n",
        "                d.setdefault(photo+\"_0\", l)\n",
        "                d.setdefault(photo+\"_1\", l)\n",
        "    return d\n",
        "def load_features(photos, features):\n",
        "    #loading all features\n",
        "    all_features = features \n",
        "    f={}\n",
        "    #selecting only needed features\n",
        "    # features = {k:all_features.get(k) for k in photos}\n",
        "    for k in photos:\n",
        "      f[k+\"_0\"] = all_features[k+\"_0\"]\n",
        "      f[k+\"_1\"] = all_features[k+\"_1\"]\n",
        "    return f\n",
        "def create_tokenizer(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(desc_list)\n",
        "    return tokenizer\n",
        "#calculate maximum length of descriptions\n",
        "def max_length(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    return max(len(d.split()) for d in desc_list)\n",
        "    \n",
        "# Training data from file\n",
        "train_photos   = load_photos(\"/content/drive/MyDrive/Colab Notebooks/data/Flickr_8k.trainImages.txt\")\n",
        "print(len(train_photos))\n",
        "# Training description from Descriptions dictionary\n",
        "train_desc     = load_descriptions(train_photos)\n",
        "print(len(train_desc))\n",
        "# Extract training image features from extracted features\n",
        "train_features = load_features(train_photos, features)\n",
        "print(len(train_features))\n",
        "\n",
        "# Validation data from file\n",
        "val_photos   = load_photos(\"/content/drive/MyDrive/Colab Notebooks/data/Flickr_8k.devImages.txt\")\n",
        "# Training description from Descriptions dictionary\n",
        "val_desc     = load_descriptions(val_photos)\n",
        "# Extract training image features from extracted features\n",
        "val_features = load_features(val_photos, features)\n",
        "\n",
        "# Convert training descriptions to list \n",
        "desc_list = dict_to_list(train_desc)\n",
        "# Create a tokenizer which has all training unique words with coresponding number\n",
        "tokenizer = create_tokenizer(train_desc)\n",
        "vocab_size = len(tokenizer.word_counts) + 1\n",
        "max_length = max_length(descriptions)\n",
        "max_length"
      ],
      "metadata": {
        "id": "2WugI8unJgce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator_batch_augmented(descriptions, features, tokenizer, max_length, batch_size=32):\n",
        "    counter = 0\n",
        "    features_list = list()\n",
        "    desc_list = list()\n",
        "    while True:\n",
        "        for key, description_list in descriptions.items():\n",
        "            #retrieve photo features\n",
        "            feature = features[key][0]\n",
        "            features_list.append(feature)\n",
        "            if desc_list == []:\n",
        "              desc_list = description_list\n",
        "            else:\n",
        "              desc_list.append(description_list)\n",
        "            counter += 1\n",
        "            if counter == batch_size:\n",
        "                counter = 0\n",
        "                input_image, input_sequence, output_word = create_sequences_batch_augmented(tokenizer, max_length, desc_list, features_list)\n",
        "                yield [[input_image, input_sequence], output_word]\n",
        "                features_list = list()\n",
        "                desc_list = list()            \n",
        "\n",
        "def create_sequences_batch_augmented(tokenizer, max_length, desc_list, feature, batch_size = 32):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    # walk through each description for the image\n",
        "    for idx, desc in enumerate(desc_list):\n",
        "        # encode the sequence\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        # seq = text_to_sequence(tokenizer, desc)\n",
        "        # split one sequence into multiple X,y pairs\n",
        "        for i in range(1, len(seq)):\n",
        "            # split into input and output pair\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            # pad input sequence\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            # encode output sequence\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            # store\n",
        "            X1.append(feature[idx//5])\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "#You can check the shape of the input and output for your model\n",
        "# [a,b],c = next(data_generator(train_desc, features, tokenizer, max_length))\n",
        "# a.shape, b.shape, c.shape\n",
        "#((47, 2048), (47, 32), (47, 7577))\n",
        "#((50, 2048), (50, 38), (50, 7321))"
      ],
      "metadata": {
        "id": "hvaCZ4xiJ4-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import add, SimpleRNN\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    # inputs1 = Input((299,299,1))\n",
        "    # Including the Xception model without the final layer\n",
        "    # fe1 = Xception(include_top = False, pooling='avg' )(inputs1)    \n",
        "    # Setting only the final layers as trainable\n",
        "    # for layer in fe1.layers[:-36]:\n",
        "    #   layer.trainable = False\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu',kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(fe1)\n",
        "    # LSTM sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(se2)\n",
        "    se4 = Dropout(0.5)(se3)\n",
        "    # Merging both models\n",
        "    decoder1 = add([fe2, se4])\n",
        "    decoder2 = Dense(128, activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.reset_states()\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "metadata": {
        "id": "k5ASdOKhKE9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "print('Dataset: ', len(train_photos))\n",
        "print('Descriptions: train=', len(train_desc))\n",
        "print('Photos: train=', len(train_features))\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Description Length: ', max_length)\n",
        "model = define_model(vocab_size, max_length)\n",
        "# epochs = 10\n",
        "# steps = len(train_desc)\n",
        "# steps_val = len(val_desc)\n",
        "\n",
        "# for i in range(epochs):\n",
        "#     generator_train = data_generator(train_desc, train_features, tokenizer, max_length)\n",
        "#     generator_val = data_generator(val_desc, val_features, tokenizer, max_length)\n",
        "#     model.fit(generator_train, validation_data= generator_val, epochs=1, steps_per_epoch= steps, validation_steps= steps_val, verbose=1, callbacks = [tensorboard_callback])\n",
        "#     model.save(\"model_\" + str(i) + \".h5\")\n",
        "epochs = 10\n",
        "BATCH = 1\n",
        "steps = len(train_desc)// BATCH\n",
        "steps_val = len(val_desc) // BATCH\n",
        "\n",
        "for i in range(epochs):\n",
        "    generator_train = data_generator_batch_augmented(train_desc, train_features, tokenizer, max_length, BATCH)\n",
        "    generator_val = data_generator_batch_augmented(val_desc, val_features, tokenizer, max_length, BATCH)\n",
        "    model.fit(generator_train, validation_data= generator_val, epochs=1, steps_per_epoch= steps, validation_steps= steps_val, verbose=1)\n",
        "    model.save(\"model_\" + str(i) + \".h5\")"
      ],
      "metadata": {
        "id": "vP9rPW26KRp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/features_aug2.p')\n",
        "files.download('/content/model_9.h5')"
      ],
      "metadata": {
        "id": "YT5VQpnZKW6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import Xception\n",
        "\n",
        "def extract_features(filename, model):\n",
        "        try:\n",
        "            image = Image.open(filename)\n",
        "\n",
        "        except:\n",
        "            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n",
        "        image = image.resize((299,299))\n",
        "        image = np.array(image)\n",
        "        # for images that has 4 channels, we convert them into 3 channels\n",
        "        if image.shape[2] == 4: \n",
        "            image = image[..., :3]\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = image/127.5\n",
        "        image = image - 1.0\n",
        "        feature = model.predict(image)\n",
        "        return feature\n",
        "\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    \n",
        "    # for word, index in tokenizer.items():\n",
        "    #     if index == integer:\n",
        "    #         return word\n",
        "    return None\n",
        "\n",
        "\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'start'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # sequence = text_to_sequence(tokenizer, in_text)\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        pred = model.predict([photo,sequence], verbose=0)\n",
        "        pred = np.argmax(pred)\n",
        "        word = word_for_id(pred, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'end':\n",
        "            break\n",
        "    return in_text.replace(\"start \",\"\").replace(\" end\",\"\").capitalize() + \".\"\n",
        "\n",
        "\n",
        "#path = 'Flicker8k_Dataset/111537222_07e56d5a30.jpg'\n",
        "max_length = 38\n",
        "# tokenizer = load(open(\"/content/tokenizer_keras_reg.p\",\"rb\"))#/content/drive/MyDrive/Colab Notebooks/data/Models/tokenizer.p\n",
        "model = load_model('/content/model_9 (1).h5')\n",
        "# model = load_model('/content/model_9.h5')#'/content/drive/MyDrive/Colab Notebooks/data/model_19_kerasTokenizer.h5')#/content/drive/MyDrive/Colab Notebooks/data/Models/model_9.h5')#/content/drive/MyDrive/Colab Notebooks/data/model_9.h5\n",
        "xception_model = Xception(include_top=False, pooling=\"avg\")\n",
        "\n",
        "photo = extract_features(\"/content/drive/MyDrive/Colab Notebooks/data/Images/1000268201_693b08cb0e.jpg\", xception_model)#/content/drive/MyDrive/Colab Notebooks/data/Images/111537222_07e56d5a30.jpg\n",
        "img = Image.open(\"/content/drive/MyDrive/Colab Notebooks/data/Images/1000268201_693b08cb0e.jpg\")\n",
        "\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "gZqNGvMNKbPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "import random\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "\n",
        "smooth_func = SmoothingFunction().method1\n",
        "\n",
        "file = open('/content/drive/MyDrive/Colab Notebooks/data/Flickr_8k.testImages.txt',\"r\")\n",
        "test_images = []\n",
        "\n",
        "for filename in file.read().split(\"\\n\")[:-1]:\n",
        "    test_images.append(filename)\n",
        "# model = load_model(\"models\\\\model2_19.h5\")\n",
        "for i in range(0,5):\n",
        "    rand_idx = random.randint(0, len(test_images)-1)\n",
        "    fname = \"/content/drive/MyDrive/Colab Notebooks/data/Images/\" + test_images[rand_idx]\n",
        "    photo = extract_features (fname, xception_model)\n",
        "    img = Image.open(fname)\n",
        "    disc = generate_desc(model, tokenizer, photo, max_length)\n",
        "    print(\"\\n\\n\")\n",
        "    # Prediction\n",
        "    print(\"Prediction: {}\".format(disc))\n",
        "    print()\n",
        "    # Description\n",
        "    score = sentence_bleu(descriptions[test_images[rand_idx]], disc, smoothing_function=smooth_func)\n",
        "    for desc in descriptions[test_images[rand_idx]]:\n",
        "        print(\"Reference: {} \".format(desc))\n",
        "    print(\"BLEU: {}\".format(score))  \n",
        "    # tokenizer.tokenize()\n",
        "    # print(descriptions[test_images[rand_idx]])\n",
        "    # print(\"METEOR: {}\".format(meteor_score(descriptions[test_images[rand_idx]], str(description))))\n",
        "    # Sentence BLEU\n",
        "    # total_score.append(score)\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.show(block=True)"
      ],
      "metadata": {
        "id": "QbPYuUDEK1OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/Colab Notebooks/data/Flickr_8k.testImages.txt\", \"r\")\n",
        "predictions = []\n",
        "# Extracting predictions\n",
        "for filename in file.read().split(\"\\n\")[:-1]:\n",
        "    photo = extract_features(\"/content/drive/MyDrive/Colab Notebooks/data/Images/\" + filename, xception_model)\n",
        "    description = generate_desc(model, tokenizer, photo, max_length)\n",
        "    predictions.append(description)\n",
        "predictions    "
      ],
      "metadata": {
        "id": "1iaJrmRTK6H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score as bscore\n",
        "file = open(\"/content/drive/MyDrive/Colab Notebooks/data/Flickr_8k.testImages.txt\", \"r\")\n",
        "bert_score_results = {}\n",
        "for idx, filename in enumerate(file.read().split(\"\\n\")[:-1]):\n",
        "  scores = []\n",
        "  for desc in descriptions[filename]:\n",
        "    _, _, score = bscore([desc], [predictions[idx]], lang='en', verbose=False)\n",
        "    scores.append(score.item())\n",
        "  mean_score = np.mean(scores)\n",
        "  bert_score_results.setdefault(filename, (predictions[idx],mean_score))\n",
        "bert_score_results"
      ],
      "metadata": {
        "id": "zhNyTjK9LC_0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}