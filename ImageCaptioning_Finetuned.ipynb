{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1x8b3K4PgDPL6VcmAdAmJuwKfM5YNbRgA",
      "authorship_tag": "ABX9TyOs3Jqdh6QtMWHeQBWZ7d6d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnandVishnuG/ImageCaptioningLSTM/blob/main/ImageCaptioning_Finetuned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons\n",
        "!pip install bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2xmL9hRaP9v",
        "outputId": "ac077bd9-223b-4580-d6a3-a3c823533a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow_addons) (23.0)\n",
            "Collecting typeguard>=2.7\n",
            "  Downloading typeguard-3.0.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow_addons) (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow_addons) (6.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.7->tensorflow_addons) (3.15.0)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.19.0 typeguard-3.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from bert_score) (1.4.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from bert_score) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from bert_score) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.9/dist-packages (from bert_score) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from bert_score) (23.0)\n",
            "Collecting transformers>=3.0.0\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from bert_score) (1.13.1+cu116)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.1->bert_score) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.0.0->bert_score) (4.5.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.0->bert_score) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.0->bert_score) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=3.0.0->bert_score) (2022.10.31)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->bert_score) (1.0.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->bert_score) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->bert_score) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->bert_score) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->bert_score) (3.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->bert_score) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, bert_score\n",
            "Successfully installed bert_score-0.3.13 huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZT_mf1ENW-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006bd89c-c01d-4b59-bfb6-0c899f6c285c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from google.colab import files, drive\n",
        "from tqdm import tqdm \n",
        "from PIL import Image\n",
        "from pickle import dump, load\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Input, Rescaling, Dense, Flatten, LSTM, Embedding, Dropout, add\n",
        "from keras.applications import Xception\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences, to_categorical, plot_model\n",
        "from tensorflow.io import read_file\n",
        "from tensorflow.image import decode_jpeg, resize, convert_image_dtype\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow_addons.optimizers import MultiOptimizer\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google drive for accessing files\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wWedbd3xPMSm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "917d2fa7-9826-4cc2-d719-ddd3163d7b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9304b8a60a4e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mount Google drive for accessing files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Captions file and create descriptions dictionary\n",
        "def read_captions(filename):\n",
        "# Reading captions from file\n",
        "  captions = pd.read_csv(filename,encoding=\"cp1252\")\n",
        "  return captions\n",
        "\n",
        "# Create a dictionary of images as keys and each image's descriptions as items\n",
        "def data_to_dictionary(data):\n",
        "    descriptions = {}\n",
        "\n",
        "    for idx, row in data.iterrows():\n",
        "        if row[\"image\"] not in descriptions:\n",
        "            descriptions.setdefault(row[\"image\"], []).append(row[\"caption\"])\n",
        "        else:\n",
        "            descriptions[row[\"image\"]].append(row[\"caption\"])\n",
        "    return descriptions\n",
        "# Create vocabulary of all words used in captions\n",
        "def text_vocabulary(descriptions):\n",
        "    vocab = set()\n",
        "    for key in descriptions.keys():\n",
        "        [vocab.update(d.split()) for d in descriptions[key]]\n",
        "    return vocab\n",
        "#converting dictionary to clean list of descriptions\n",
        "def dict_to_list(descriptions):\n",
        "    all_desc = []\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "# Return list of image names to fetch\n",
        "def load_photos(filename):\n",
        "    file = open(filename, 'r')\n",
        "    photos = file.read().split(\"\\n\")[:-1]\n",
        "    return photos\n",
        "# Return descriptions for the image names, padded with <start> and <end> \n",
        "def load_descriptions(photos):\n",
        "    d = {}\n",
        "    for photo in photos:\n",
        "        if photo in descriptions.keys():\n",
        "            if photo not in d.keys():\n",
        "                l = []            \n",
        "                for desc in descriptions[photo]:\n",
        "                    l.append(\"<start> \" + desc + \" <end>\" ) \n",
        "                d.setdefault(photo, l)\n",
        "    return d\n",
        "\n",
        "# Vectorize the data\n",
        "def create_tokenizer(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(desc_list)\n",
        "    return tokenizer\n",
        "#Calculate maximum length of descriptions\n",
        "def max_length(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    return max(len(d.split()) for d in desc_list)\n"
      ],
      "metadata": {
        "id": "pQz_ue2ON28D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "    # Defining base model with no top layer\n",
        "    base_model = Xception(input_shape=(299, 299, 3), include_top=False, pooling='avg') \n",
        "    # Setting training mode to False for all except the last 10 layers\n",
        "    for layer in base_model.layers[:-10]:\n",
        "        layer.trainable = False\n",
        "    # Scaling images for optimization\n",
        "    scale_layer = Rescaling(scale=1 / 127.5, offset=-1)\n",
        "    # Input images of size 299 x 299 and 3 Channels\n",
        "    input1 = Input((299,299,3))\n",
        "    x1 = Rescaling(scale=1/127.5, offset= -1)(input1)\n",
        "    # x1 = scale_layer(input1)\n",
        "    x1 = base_model(x1)\n",
        "    x1 = Flatten()(x1)\n",
        "    # Regularizing with drop out\n",
        "    x1 = Dropout(0.5)(x1)\n",
        "    # Intermediate output layer of 256 features \n",
        "    x1 = Dense(256, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001), activation='relu')(x1)\n",
        "    # LSTM sequence model\n",
        "    # Array of maximum length of descriptions\n",
        "    input2 = Input(shape=(max_length,))\n",
        "    # Embedding input of max_length dimensions into 256 dimensions output\n",
        "    x2 = Embedding(vocab_size, 256, mask_zero=True)(input2)\n",
        "    # Regularizing with dropout\n",
        "    x2 = Dropout(0.5)(x2)\n",
        "    # LSTM layer 256 \n",
        "    x2 = LSTM(256, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(x2)\n",
        "    # Regularizing with dropout\n",
        "    x2 = Dropout(0.5)(x2)\n",
        "    # Merging both models\n",
        "    x = add([x1, x2])\n",
        "    # 128 neurons\n",
        "    x = Dense(128, activation='relu',kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(x)\n",
        "    # Output layer\n",
        "    output = Dense(vocab_size, activation='softmax')(x)\n",
        "    # Combine into one model [image, seq] [word]\n",
        "    model = Model(inputs=[input1, input2], outputs=output)\n",
        "    # Initialize optimizers with different rates for base_model and fully connected layers\n",
        "    optimizer_base_model_layers = Adam(learning_rate=0.0001)\n",
        "    optimizer_fully_connected_layers = Adam()\n",
        "    # Setting optimizers for each layer\n",
        "    optimizers_and_layers = [(optimizer_base_model_layers, model.layers[3]), \n",
        "                            (optimizer_fully_connected_layers, model.layers[4:])]\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=MultiOptimizer(optimizers_and_layers))\n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    # Plotting the model\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "metadata": {
        "id": "2JcObVuXPbNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading captions\n",
        "captions = read_captions(\"/content/drive/MyDrive/Colab Notebooks/data/captions_clean.txt\")\n",
        "# Creating dictionary for descriptions\n",
        "descriptions = data_to_dictionary(captions)\n"
      ],
      "metadata": {
        "id": "iSaoub09OE7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"/content/model_10_finetuned.h5\")\n",
        "# tokenizer = load(open(\"/content/tokenizer (1).p\",\"rb\"))\n"
      ],
      "metadata": {
        "id": "fQOPZCUJy4IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "train_photos   = load_photos(\"/content/drive/MyDrive/Colab Notebooks/data/Flickr_8k.trainImages.txt\")\n",
        "train_desc     = load_descriptions(train_photos)\n",
        "# Valiation data\n",
        "val_photos   = load_photos(\"/content/drive/MyDrive/Colab Notebooks/data/Flickr_8k.devImages.txt\")\n",
        "val_desc     = load_descriptions(train_photos)\n",
        "# Test data\n",
        "test_photos   = load_photos(\"/content/drive/MyDrive/Colab Notebooks/data/Flickr_8k.testImages.txt\")\n",
        "test_desc     = load_descriptions(train_photos)\n",
        "\n",
        "# Convert training descriptions to list \n",
        "# desc_list = dict_to_list(train_desc)\n",
        "# Create tokens from the training data\n",
        "tokenizer = create_tokenizer(train_desc)\n",
        "vocab_size = len(tokenizer.word_counts) + 1\n",
        "# Description length\n",
        "# max_length = max_length(descriptions)\n",
        "print(f\"Number of training images: {len(train_photos)} \")\n",
        "print(f\"Number of training descriptions: {len(train_desc)} \")\n",
        "print(f\"Vocabulary: {vocab_size} \")\n",
        "# print(f\"Description length: {max_length} \")"
      ],
      "metadata": {
        "id": "KiSdA_kua5Kx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d3e6e9-c03b-4761-f28d-baf9ee148435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training images: 6000 \n",
            "Number of training descriptions: 6000 \n",
            "Vocabulary: 6965 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
        "    "
      ],
      "metadata": {
        "id": "SkQRyfHta4rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create input-output sequence pairs from the image description.\n",
        "#data generator, used by model.fit_generator()\n",
        "def data_generator(descriptions, tokenizer, max_length):\n",
        "    while 1:\n",
        "        for key, description_list in descriptions.items():\n",
        "            #retrieve photo features\n",
        "            img = tf.io.read_file(\"/content/drive/MyDrive/Colab Notebooks/data/Images/\" + key)\n",
        "            img = tf.image.decode_jpeg(img, channels=3)\n",
        "            img = tf.image.resize(img, (299,299))\n",
        "            img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, img)\n",
        "            yield [[input_image, input_sequence], output_word]\n",
        "\n",
        "def create_sequences(tokenizer, max_length, desc_list, image):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    # walk through each description for the image\n",
        "    for desc in desc_list:\n",
        "        # encode the sequence\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        # split one sequence into multiple X,y pairs\n",
        "        for i in range(1, len(seq)):\n",
        "            # split into input and output pair\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            # pad input sequence\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            # encode output sequence\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            # store\n",
        "            X1.append(image)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)"
      ],
      "metadata": {
        "id": "AvLT2OU1eEJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model\n",
        "model = define_model(vocab_size, max_length)\n",
        "epochs = 10\n",
        "steps = len(train_desc)\n",
        "steps_val = len(val_desc)\n",
        "histories = []\n",
        "\n",
        "for i in range(epochs):\n",
        "    generator_train = data_generator(train_desc, tokenizer, max_length)\n",
        "    generator_val = data_generator(val_desc, tokenizer, max_length)\n",
        "    history = model.fit(generator_train, validation_data= generator_val, epochs=1, steps_per_epoch= steps, validation_steps= steps_val, verbose=1)\n",
        "    histories.append(history)\n",
        "    model.save(\"model_\" + str(i+1) + \".h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6B9esY6cDWd",
        "outputId": "bd5d6545-4980-446e-8857-0758e3dd7d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 299, 299, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " rescaling_7 (Rescaling)        (None, 299, 299, 3)  0           ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " xception (Functional)          (None, 2048)         20861480    ['rescaling_7[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 38, 256)      1783040     ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 2048)         0           ['xception[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 38, 256)      0           ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 2048)         0           ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  (None, 256)          525312      ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 256)          524544      ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 256)          0           ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " add_51 (Add)                   (None, 256)          0           ['dense_9[0][0]',                \n",
            "                                                                  'dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 128)          32896       ['add_51[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 6965)         898485      ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 24,625,757\n",
            "Trainable params: 8,515,125\n",
            "Non-trainable params: 16,110,632\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000/6000 [==============================] - 2658s 440ms/step - loss: 4.5714 - val_loss: 4.1384\n",
            "6000/6000 [==============================] - 1563s 261ms/step - loss: 3.8903 - val_loss: 3.8996\n",
            "6000/6000 [==============================] - 1565s 261ms/step - loss: 3.6710 - val_loss: 3.7562\n",
            "6000/6000 [==============================] - 1564s 261ms/step - loss: 3.5580 - val_loss: 3.6548\n",
            "6000/6000 [==============================] - 1566s 261ms/step - loss: 3.4815 - val_loss: 3.6140\n",
            "6000/6000 [==============================] - 1566s 261ms/step - loss: 3.4253 - val_loss: 3.5841\n",
            "6000/6000 [==============================] - 1567s 261ms/step - loss: 3.3750 - val_loss: 3.5443\n",
            "6000/6000 [==============================] - 1569s 262ms/step - loss: 3.3373 - val_loss: 3.5493\n",
            "6000/6000 [==============================] - 1567s 261ms/step - loss: 3.3013 - val_loss: 3.5323\n",
            "6000/6000 [==============================] - 1566s 261ms/step - loss: 3.2736 - val_loss: 3.5067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/model_10.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pFvGBH1PmAf1",
        "outputId": "db01a0e4-e6b1-4c6f-a9c6-0b183f6af79e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e29cefd2-a314-42db-ad5f-fefea80af20f\", \"model_10.h5\", 98802308)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dump(histories, open('histories.p', 'wb'))\n",
        "files.download('/content/tokenizer.p')\n",
        "files.download('/content/histories.p')"
      ],
      "metadata": {
        "id": "sorTnN5XU8xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"/content/model_10_finetuned.h5\")\n",
        "# tokenizer_keras_reg = load(open(\"/content/tokenizer_keras_reg.p\",\"rb\"))\n",
        "# features = load(open(\"/content/tokenizer (1).p\",\"rb\"))"
      ],
      "metadata": {
        "id": "5q24wqdXt7_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuUFz-9aulKK",
        "outputId": "6ed807ab-22c4-4a61-bef7-bd482df2be73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, 299, 299, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " rescaling_7 (Rescaling)        (None, 299, 299, 3)  0           ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, 38)]         0           []                               \n",
            "                                                                                                  \n",
            " xception (Functional)          (None, 2048)         20861480    ['rescaling_7[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 38, 256)      1783040     ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 2048)         0           ['xception[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 38, 256)      0           ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 2048)         0           ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  (None, 256)          525312      ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 256)          524544      ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 256)          0           ['lstm_3[0][0]']                 \n",
            "                                                                                                  \n",
            " add_51 (Add)                   (None, 256)          0           ['dense_9[0][0]',                \n",
            "                                                                  'dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 128)          32896       ['add_51[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 6965)         898485      ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 24,625,757\n",
            "Trainable params: 8,515,125\n",
            "Non-trainable params: 16,110,632\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'start'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        pred = model.predict([photo,sequence], verbose=0)\n",
        "        pred = np.argmax(pred)\n",
        "        word = word_for_id(pred, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'end':\n",
        "            break\n",
        "    return in_text.replace(\"start \",\"\").replace(\" end\",\"\").capitalize() + \".\"\n"
      ],
      "metadata": {
        "id": "KBG8sreq0nFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import read_csv\n",
        "import random\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from bert_score import score as BScore\n",
        "\n",
        "smooth_func = SmoothingFunction().method1\n",
        "\n",
        "\n",
        "for i in range(0,5):\n",
        "    rand_idx = random.randint(0, len(test_photos)-1)\n",
        "    img = tf.io.read_file(\"/content/drive/MyDrive/Colab Notebooks/data/Images/\" + test_photos[rand_idx])\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    image = img\n",
        "    img = tf.image.resize(img, (299,299))\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "\n",
        "    description = generate_desc(model, tokenizer, np.array(img), max_length)\n",
        "    print(\"\\n\\n\")\n",
        "    # Prediction\n",
        "    print(\"Prediction: {}\".format(description))\n",
        "    print()\n",
        "    # Description\n",
        "    score = sentence_bleu(descriptions[test_photos[rand_idx]], description, smoothing_function=smooth_func)\n",
        "    for desc in descriptions[test_photos[rand_idx]]:\n",
        "        print(\"Reference: {} \".format(desc))\n",
        "    print(\"BLEU: {}\".format(score))  \n",
        "    refs = []\n",
        "    refs.append(description)\n",
        "    refs.append(description)\n",
        "    refs.append(description)\n",
        "    refs.append(description)\n",
        "    refs.append(description)\n",
        "    p, r, f1 = BScore(refs, descriptions[test_photos[rand_idx]], lang='en', verbose=False)\n",
        "    print(\"BERTScore: {:.4f}\".format(f1.mean().item()))\n",
        "    # tokenizer.tokenize()\n",
        "    # print(descriptions[test_images[rand_idx]])\n",
        "    # print(\"METEOR: {}\".format(meteor_score(descriptions[test_images[rand_idx]], str(description))))\n",
        "    # Sentence BLEU\n",
        "    # total_score.append(score)\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.show(block=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wJZ-aZX2zZoZ",
        "outputId": "df327164-47a2-46ba-dfd3-3fd5b858acab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-9fc14ab2c9bf>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-446604f93fc9>\u001b[0m in \u001b[0;36mgenerate_desc\u001b[0;34m(model, tokenizer, photo, max_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphoto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_for_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_3/xception/block1_conv1/Conv2D' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-23-9fc14ab2c9bf>\", line 20, in <cell line: 11>\n      description = generate_desc(model, tokenizer, np.array(img), max_length)\n    File \"<ipython-input-22-446604f93fc9>\", line 12, in generate_desc\n      pred = model.predict([photo,sequence], verbose=0)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2382, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/convolutional/base_conv.py\", line 290, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/convolutional/base_conv.py\", line 262, in convolution_op\n      return tf.nn.convolution(\nNode: 'model_3/xception/block1_conv1/Conv2D'\nDNN library is not found.\n\t [[{{node model_3/xception/block1_conv1/Conv2D}}]] [Op:__inference_predict_function_17476]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_keras_reg = load_model(\"/content/model_9keras_reg.h5\")\n",
        "tokenizer_keras_reg = load(open(\"/content/tokenizer_keras_reg.p\",\"rb\"))\n",
        "# features = load(open(\"/content/tokenizer (1).p\",\"rb\"))"
      ],
      "metadata": {
        "id": "yA1ErKALWYzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import Xception\n",
        "\n",
        "def extract_features(filename, model):\n",
        "        try:\n",
        "            image = Image.open(filename)\n",
        "\n",
        "        except:\n",
        "            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n",
        "        image = image.resize((299,299))\n",
        "        image = np.array(image)\n",
        "        # for images that has 4 channels, we convert them into 3 channels\n",
        "        if image.shape[2] == 4: \n",
        "            image = image[..., :3]\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = image/127.5\n",
        "        image = image - 1.0\n",
        "        feature = model.predict(image)\n",
        "        return feature\n",
        "\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    \n",
        "    # for word, index in tokenizer.items():\n",
        "    #     if index == integer:\n",
        "    #         return word\n",
        "    return None\n",
        "\n",
        "\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'start'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # sequence = text_to_sequence(tokenizer, in_text)\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        pred = model.predict([photo,sequence], verbose=0)\n",
        "        pred = np.argmax(pred)\n",
        "        word = word_for_id(pred, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'end':\n",
        "            break\n",
        "    return in_text.replace(\"start \",\"\").replace(\" end\",\"\").capitalize() + \".\"\n",
        "\n",
        "from pandas import read_csv\n",
        "import random\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "\n",
        "smooth_func = SmoothingFunction().method1\n",
        "\n",
        "file = open('/content/drive/MyDrive/Colab Notebooks/data/Flickr_8k.testImages.txt',\"r\")\n",
        "test_images = []\n",
        "\n",
        "for filename in file.read().split(\"\\n\")[:-1]:\n",
        "    test_images.append(filename)\n",
        "# model = load_model(\"models\\\\model2_19.h5\")\n",
        "xception_model = Xception(include_top=False, pooling=\"avg\")\n",
        "for i in range(0,5):\n",
        "    rand_idx = random.randint(0, len(test_images)-1)\n",
        "    fname = \"/content/drive/MyDrive/Colab Notebooks/data/Images/\" + test_images[rand_idx]\n",
        "    \n",
        "    photo = extract_features (fname, xception_model)\n",
        "    img = Image.open(fname)\n",
        "    description = generate_desc(model, tokenizer, photo, max_length)\n",
        "    print(\"\\n\\n\")\n",
        "    # Prediction\n",
        "    print(\"Prediction: {}\".format(description))\n",
        "    print()\n",
        "    # Description\n",
        "    score = sentence_bleu(descriptions[test_images[rand_idx]], description, smoothing_function=smooth_func)\n",
        "    for desc in descriptions[test_images[rand_idx]]:\n",
        "        print(\"Reference: {} \".format(desc))\n",
        "    print(\"BLEU: {}\".format(score))  \n",
        "    # tokenizer.tokenize()\n",
        "    # print(descriptions[test_images[rand_idx]])\n",
        "    # print(\"METEOR: {}\".format(meteor_score(descriptions[test_images[rand_idx]], str(description))))\n",
        "    # Sentence BLEU\n",
        "    # total_score.append(score)\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.show(block=True)"
      ],
      "metadata": {
        "id": "gdEHPQdpqSc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb4a81d6-1172-4d2e-f2c1-07810f938e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 4s 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-00ef406f8df8>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/data/Images/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mphoto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxception_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-00ef406f8df8>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(filename, model)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m127.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'xception/block1_conv1/Conv2D' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-27-00ef406f8df8>\", line 67, in <cell line: 63>\n      photo = extract_features (fname, xception_model)\n    File \"<ipython-input-27-00ef406f8df8>\", line 17, in extract_features\n      feature = model.predict(image)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2382, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/convolutional/base_conv.py\", line 290, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/layers/convolutional/base_conv.py\", line 262, in convolution_op\n      return tf.nn.convolution(\nNode: 'xception/block1_conv1/Conv2D'\nDNN library is not found.\n\t [[{{node xception/block1_conv1/Conv2D}}]] [Op:__inference_predict_function_23708]"
          ]
        }
      ]
    }
  ]
}